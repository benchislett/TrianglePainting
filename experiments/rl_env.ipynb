{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71984614-685c-4cab-a25d-9452d738861f",
   "metadata": {},
   "source": [
    "## Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545dbc8-1fc4-42ac-86bd-921cd469b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Paint\n",
    "using Serialization\n",
    "using Images, ImageShow\n",
    "using Plots\n",
    "using StaticArrays\n",
    "using ImageFeatures\n",
    "using IntervalSets\n",
    "using ReinforcementLearning\n",
    "using ReinforcementLearningBase\n",
    "using ReinforcementLearningZoo\n",
    "using Random\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "import Flux: params\n",
    "using Distributions: Normal\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096b11a-238e-44e4-8cea-646dfef2143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = float.(load(\"../lisa.png\"))\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047336ed-a2fe-4953-870a-56e22e3779cf",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "State is a 10x10 downsampled image of the difference map, Action is a vector of length 6 corresponding to a triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca7295-1847-4a28-9655-596eacc38fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NTris = 10\n",
    "\n",
    "ns = 10 * 10 # number of states\n",
    "na = 6 # number of \"actions\" \n",
    "\n",
    "mutable struct MyEnv <: AbstractEnv\n",
    "    target::Array{RGB{Float32}, 2}\n",
    "    img::Array{RGB{Float32}, 2}\n",
    "    idx::Int\n",
    "end\n",
    "MyEnv(targetimg) = MyEnv(targetimg, zero(targetimg) .+ averagepixel(targetimg), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765b3cc-a938-4b13-bf9b-7d0c5dba6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReinforcementLearningBase.action_space(env::MyEnv) = Space([0.0..1.0, 0.0..1.0, 0.0..1.0, 0.0..1.0, 0.0..1.0, 0.0..1.0])\n",
    "ReinforcementLearningBase.state_space(env::MyEnv) = Space([0.0..1.0 for _ in ns])\n",
    "\n",
    "function ReinforcementLearningBase.state(env::MyEnv)\n",
    "    diff = Gray.(abs.(env.img .- env.target))\n",
    "    diff = diff ./ maximum(diff)\n",
    "    \n",
    "    ret_state = zeros(Float32, 100)\n",
    "    for i = 1:10\n",
    "        for j = 1:10\n",
    "            ret_state[(i - 1) * 10 + j]= sum(diff[ 20*(i-1)+1:20*i, 20*(j-1)+1:20*j ]) / Float32(20*20)\n",
    "        end\n",
    "    end\n",
    "    ret_state\n",
    "end\n",
    "\n",
    "function ReinforcementLearningBase.reward(env::MyEnv)\n",
    "    - imloss(env.img, env.target, SELoss())\n",
    "end\n",
    "\n",
    "ReinforcementLearningBase.is_terminated(env::MyEnv) = (env.idx == NTris)\n",
    "\n",
    "function ReinforcementLearningBase.reset!(env::MyEnv)\n",
    "    env.img = zero(env.target) .+ averagepixel(env.target)\n",
    "    env.idx = 0\n",
    "\n",
    "    env\n",
    "end\n",
    "\n",
    "function (env::MyEnv)(action)\n",
    "    env.idx += 1\n",
    "    tri = Triangle(SVector{6, Float32}(action))\n",
    "    col = averagepixel(target, tri, RasterAlgorithmScanline())\n",
    "    draw!(env.img, tri, col, RasterAlgorithmScanline())\n",
    "\n",
    "    # println(tri)\n",
    "    # println(drawloss(env.target, env.img, tri, col, SELoss(), RasterAlgorithmScanline()))\n",
    "\n",
    "    env\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cbdbab-be68-4824-b99c-a26d01e2ae0c",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Agent is a Soft Actor-Critic based on a small MLP with Q-Net, on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f4c8a-5ef1-4182-8ec5-d8a40315c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = Random.GLOBAL_RNG\n",
    "init = glorot_uniform(rng)\n",
    "\n",
    "create_policy_net() = NeuralNetworkApproximator(\n",
    "    model=GaussianNetwork(\n",
    "        pre=Chain(\n",
    "            Dense(ns, 30, relu, init=init),\n",
    "            Dense(30, 30, relu, init=init),\n",
    "        ),\n",
    "        μ=Chain(Dense(30, na, init=init)),\n",
    "        logσ=Chain(Dense(30, na, x -> clamp(x, typeof(x)(-10), typeof(x)(2)), init=init)),\n",
    "    ),\n",
    "    optimizer=ADAM(0.003),\n",
    ") |> gpu\n",
    "\n",
    "create_q_net() = NeuralNetworkApproximator(\n",
    "    model=Chain(\n",
    "        Dense(ns + na, 30, relu; init=init),\n",
    "        Dense(30, 30, relu; init=init),\n",
    "        Dense(30, 1; init=init),\n",
    "    ),\n",
    "    optimizer=ADAM(0.003),\n",
    ") |> gpu\n",
    "\n",
    "agent = Agent(\n",
    "    policy=SACPolicy(\n",
    "        policy=create_policy_net(),\n",
    "        qnetwork1=create_q_net(),\n",
    "        qnetwork2=create_q_net(),\n",
    "        target_qnetwork1=create_q_net(),\n",
    "        target_qnetwork2=create_q_net(),\n",
    "        γ=0.99f0,\n",
    "        τ=0.005f0,\n",
    "        α=0.2f0,\n",
    "        batch_size=64,\n",
    "        start_steps=1000,\n",
    "        start_policy=RandomPolicy(Space([0.0 .. 1.0 for _ in 1:na]); rng=rng),\n",
    "        update_after=1000,\n",
    "        update_freq=1,\n",
    "        automatic_entropy_tuning=true,\n",
    "        lr_alpha=0.003f0,\n",
    "        action_dims=1,\n",
    "        rng=rng,\n",
    "        device_rng=CUDA.functional() ? CUDA.CURAND.RNG() : rng\n",
    "    ),\n",
    "    trajectory=CircularArraySARTTrajectory(\n",
    "        capacity=10000,\n",
    "        state=Vector{Float32} => (ns,),\n",
    "        action=Vector{Float32} => (na,),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029f41c-b4b8-440a-af7e-87dc6b478a5f",
   "metadata": {},
   "source": [
    "## Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e08bc-8cd5-42e7-93d2-8fa1d94e7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e80709-d74f-4719-827f-074c0f2cf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run(\n",
    "   agent,\n",
    "   env,\n",
    "   StopAfterEpisode(10000),\n",
    "   TotalRewardPerEpisode()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ee45-c459-4b9a-9a18-754a81731e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b92455-eba2-4ec8-bed8-c5c9207004b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
